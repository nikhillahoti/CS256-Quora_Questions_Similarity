{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import cython\n",
    "\n",
    "import os\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "stops = None\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "word2VecDirectory_Input = os.getcwd() + \"/Word2VecData_Input/\" \n",
    "word2VecDirectory_Output = os.getcwd() + \"/Word2VecData_Output/\" \n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "modelfileName = \"CS256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWords():\n",
    "    global stops\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.add(\"what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessString(CurrentString):\n",
    "    \n",
    "    if CurrentString is None: return None\n",
    "    CurrentString = str(CurrentString)\n",
    "    CurrentString = CurrentString.translate(translator)\n",
    "    result = \"\"\n",
    "    CurrentString = CurrentString.split()\n",
    "    for word in CurrentString:\n",
    "        if word is None or word == \"\": continue\n",
    "        try:\n",
    "            word = stemmer.stem(word.lower())\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if word.lower() not in stops:\n",
    "            result += word + \" \"\n",
    "    return result\n",
    "\n",
    "def getModelData():\n",
    "    start = time.time()\n",
    "    global combinedWordSet\n",
    "    global translator\n",
    "    global stemmer\n",
    "    global stops\n",
    "    \n",
    "    getStopWords()\n",
    "\n",
    "    filenames = ['train.csv', 'test.csv']\n",
    "    columnNames = ['question1', 'question2']\n",
    "    for i in range(len(filenames)):\n",
    "        df = pd.read_csv(filenames[i])\n",
    "        for j in range(len(columnNames)):\n",
    "            fOutput = open(word2VecDirectory_Input + columnNames[j] + \".txt\", \"a\")\n",
    "            for question in df[columnNames[j]]:\n",
    "                result = preprocessString(question)\n",
    "                if result is None: continue\n",
    "                fOutput.write(result + \"\\n\")\n",
    "            fOutput.close()\n",
    "        \n",
    "        fOutput = open(word2VecDirectory_Output + \"is_duplicate.txt\", \"a\")\n",
    "        for word in df['is_duplicate']:\n",
    "            fOutput.write(str(word) + \"\\n\")\n",
    "        fOutput.close()\n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"Processing Ends\")\n",
    "    print(\"Total time needed -> \", (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IteratingClass:\n",
    "    def __init__(self, dirName):\n",
    "        self.dirName = dirName\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fName in os.listdir(self.dirName):\n",
    "            for line in open(os.path.join(self.dirName, fName)):\n",
    "                yield line.split()\n",
    "\n",
    "def createWord2VecModels():\n",
    "    start = time.time()\n",
    "    dataDirec = IteratingClass(word2VecDirectory_Input)\n",
    "    model = Word2Vec(dataDirec, size=200, window=5, min_count=1, workers=8)\n",
    "    model.save(modelfileName)\n",
    "    end = time.time()\n",
    "    print(\"\\n\\nTraining Successful for Word2Vec Model!!!\")\n",
    "    print(\"Total Time for Word2Vec model -> \", (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ends\n",
      "Total time needed ->  148.3205966949463\n"
     ]
    }
   ],
   "source": [
    "# This step takes a bit of time to process\n",
    "getModelData()\n",
    "createWord2VecModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#x_train = None\n",
    "def getNumpyArray():\n",
    "    start = time.time()\n",
    "    \n",
    "    global X\n",
    "    global Y\n",
    "    global dirNameInput\n",
    "    X = None\n",
    "    Y = None\n",
    "    \n",
    "    Word2Vec_model = Word2Vec.load(modelfileName)\n",
    "    print(\"Processing started \")\n",
    "    question1 = []\n",
    "    with open(word2VecDirectory_Input + \"question1.txt\", \"r\") as fQuestion1:\n",
    "        question1 = fQuestion1.readlines()\n",
    "    question1 = [x.strip() for x in question1] \n",
    "    \n",
    "    question2 = []\n",
    "    with open(word2VecDirectory_Input + \"question2.txt\", \"r\") as fQuestion2:\n",
    "        question2 = fQuestion2.readlines()\n",
    "    question2 = [x.strip() for x in question2] \n",
    "    \n",
    "    similar = []\n",
    "    with open(word2VecDirectory_Output + \"is_duplicate.txt\", \"r\") as fsimilar:\n",
    "        similar = fsimilar.readlines()\n",
    "    similar = [x.strip() for x in similar] \n",
    "    \n",
    "    print(len(similar))\n",
    "    print(len(question1))\n",
    "    print(len(question2))\n",
    "\n",
    "    length = len(similar) if len(similar) < len(question1) else len(question1)\n",
    "    length = length if length < len(question2) else len(question2)\n",
    "    \n",
    "    X_sub = []\n",
    "    Y_sub = []\n",
    "    for i in range(length):\n",
    "        \n",
    "        q1Vec = []\n",
    "        \n",
    "        parts = question1[i].split()\n",
    "        \n",
    "        if len(parts) < 1: continue\n",
    "        \n",
    "        currV = Word2Vec_model.wv[parts[0]]\n",
    "        currV.setflags(write=1)\n",
    "        for i in range(len(parts)):\n",
    "            currV += Word2Vec_model.wv[parts[i]]\n",
    "        currV /= len(parts)\n",
    "        q1Vec = np.array(currV)\n",
    "        \n",
    "        parts = question2[i].split()\n",
    "        if len(parts) < 1: continue\n",
    "        currV = Word2Vec_model.wv[parts[0]]\n",
    "        currV.setflags(write=1)\n",
    "        for i in range(len(parts)):\n",
    "            currV += Word2Vec_model.wv[parts[i]]\n",
    "        currV /= len(parts)\n",
    "        q2Vec = np.array(currV)\n",
    "        \n",
    "        currV = int(similar[i])\n",
    "        \n",
    "        Y_sub.append(np.array(currV))\n",
    "        X_sub.append(np.concatenate((q1Vec, q1Vec), axis=0))\n",
    "    \n",
    "    X = np.array(X_sub)\n",
    "    Y = np.array(Y_sub)\n",
    "    print(X.shape)    \n",
    "    print(Y.shape)    \n",
    "    end = time.time()\n",
    "    print(\"total time -> \", end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This step too takes a bit of time to process\n",
    "getNumpyArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createDNNModel():\n",
    "    global model     \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "createDNNModel()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,test_size=0.30, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def executeModel(eph):\n",
    "    start = time.time()\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=eph)\n",
    "    end = time.time()\n",
    "    print(\"Total Time -> \", (end - start))\n",
    "    \n",
    "    val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "    print(val_loss)\n",
    "    print(val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This step takes time too\n",
    "start = time.time()\n",
    "executeModel(10)\n",
    "end = time.time()\n",
    "print(\"Total Time -->\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
